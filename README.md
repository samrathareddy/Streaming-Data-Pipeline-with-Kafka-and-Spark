# ğŸš€ Enhanced Data Pipeline â€“ by Samratha Reddy

This is a **production-grade, personalized data pipeline project** built by **Samratha Reddy** for professional showcase and deployment. It has been customized to suit real-world scenarios in **batch and streaming data processing**, leveraging modern tools such as **Apache Spark, Kafka, Airflow, PostgreSQL, MinIO**, and more.

**GitHub Repository:** [github.com/samrathareddy/Enhanced-Data-Pipeline](https://github.com/samrathareddy/Enhanced-Data-Pipeline)

---

## ğŸ“Œ Key Enhancements & Contributions

* Refactored batch and streaming DAGs with robust error handling and schedule optimization
* Integrated parameterized Spark batch/streaming jobs
* Improved modularity in Airflow and Spark directories for ease of testing and extension
* Enhanced S3/MinIO and MongoDB data source compatibility
* Added detailed README with a developer-friendly walkthrough

---

## ğŸ› ï¸ Tools & Technologies

* **Processing:** Apache Spark (Batch + Streaming), PySpark
* **Ingestion:** Apache Kafka, Airflow, Kafka Producer Scripts
* **Storage:** PostgreSQL, MinIO (S3-compatible), MongoDB
* **Orchestration:** Apache Airflow
* **Monitoring:** Prometheus, Grafana, Great Expectations
* **ML & BI:** MLflow, Feast, Tableau-ready integration
* **DevOps:** Docker, Docker Compose, GitHub Actions, Kubernetes (optional)

---

## ğŸ‘¨â€ğŸ’» About Me â€“ Samratha Reddy

A hands-on **Data Engineer** and **Graduate Student at Cleveland State University**, I specialize in building scalable, real-time data solutions using cloud-native services and open-source tools. This project demonstrates my ability to customize and deploy complex data stacks for real business use cases including anomaly detection, batch ETL, and stream processing.

ğŸ”— Connect: [LinkedIn](https://www.linkedin.com/in/samrathareddy)

---

## âš™ï¸ Quick Start Guide

```bash
# Clone the repository
https://github.com/samrathareddy/Enhanced-Data-Pipeline.git
cd Enhanced-Data-Pipeline

# Start all services
docker-compose up --build
```

ğŸ“Œ After services are running:

* Airflow UI â†’ [http://localhost:8080](http://localhost:8080)
* Kafka & Producer Scripts â†’ in `kafka/`
* PostgreSQL & MinIO accessible via Docker
* Monitoring via Grafana and Prometheus

---

## ğŸ§© Use Cases Demonstrated

* Real-Time Data Quality Monitoring (Spark Streaming + Kafka)
* Batch Ingestion from MySQL â†’ MinIO â†’ Spark â†’ PostgreSQL
* Data Governance with Apache Atlas integration (stub)
* ML Experiment Tracking with MLflow

---

## ğŸ“ Project Structure (Simplified)

```
â”œâ”€â”€ airflow/              # DAGs and Airflow Docker setup
â”œâ”€â”€ spark/                # Batch and streaming Spark jobs
â”œâ”€â”€ kafka/                # Kafka setup and producer script
â”œâ”€â”€ monitoring/           # Prometheus and Grafana
â”œâ”€â”€ ml/                   # MLflow and Feast integrations
â”œâ”€â”€ storage/              # S3, MongoDB, Hadoop stubs
â”œâ”€â”€ terraform/            # Infrastructure as code (optional)
â”œâ”€â”€ kubernetes/           # K8s manifests (optional)
â”œâ”€â”€ docker-compose.yaml   # Docker service orchestration
```

---

## ğŸ” Final Notes

This project was fully customized and tested by **Samratha Reddy** to reflect personal development skills in building modern data platforms. It serves as a **portfolio-grade example** and can be extended for:

* Real-time analytics
* IoT ingestion
* Data Lake pipelines
* ML/AI workflows

---

**â­ Star this project if you like it â€” and feel free to fork and extend it yourself!**
